# Agent Tasks

This file tracks current and future implementation tasks for the ADK AI Data Protocol project.

## üìã Implementation Phases

**Phase 1: Âç≥Â∫ß„Å´ÂØæÂøúÔºàÊ©üËÉΩ„Éñ„É≠„ÉÉ„ÇØËß£Ê∂àÔºâ** - ‚úÖ Complete
- No blocking issues remaining (finishReason implemented)

**Phase 2: „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ÂÆâÂÆöÂåñ** - üü° In Progress
- [P2-T1] WebSocket Timeout Investigation - ‚úÖ Complete
- [P2-T2] WebSocket Bidirectional Communication
- [P2-T3] Immediate Error Detection (errorCode/errorMessage) - ‚úÖ Complete
- [P2-T4] Field Coverage Testing (Automated CI Checks) - ‚úÖ Complete
- [P2-T5] Tool Error Handling - ‚úÖ Complete
- [P2-T6] Unify Image Events to `file` Type - ‚úÖ Complete
- [P2-T7] Audio Completion Signaling - ‚úÖ Complete
- [P2-T8] message-metadata Event Implementation - ‚úÖ Complete

**Phase 3: Êñ∞Ê©üËÉΩÊ§úË®éÔºàUIË®≠Ë®àÂà§Êñ≠ÂæÖ„Å°Ôºâ** - ‚úÖ Complete
- [P3-T1] Live API Transcriptions - ‚úÖ Complete
- [P3-T2] Grounding & Citation Metadata - ‚úÖ Complete (Implemented in P2-T8)

**Phase 4: „Åù„ÅÆ‰ªñ** - üü¢ Low Priority
- [P4-T1] Interruption Signal Support (BIDI UX)
- [P4-T2] File References Support
- [P4-T3] Advanced Metadata Features
- [P4-T4] Multimodal Integration Testing
- [P4-T5] Documentation Updates

---

## Phase 2: „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£ÂÆâÂÆöÂåñ

### [P2-T1] WebSocket Timeout Investigation - ‚úÖ Complete

**Status:** ‚úÖ COMPLETE (2025-12-13)

**Original Issue:** WebSocket connection closes with "Deadline expired" error after successful PCM streaming
- 35 PCM audio chunks (111,360 bytes) sent successfully
- Connection closes before completion
- Error: `received 1011 (internal error) Deadline expired before operation could complete`

**Root Cause Analysis:**

1. **Connection Timeout:** ~10 minutes (Gemini Live API automatic timeout)
   - ADK automatically attempts reconnection
   - Requires session_resumption (Vertex AI only) for transparent reconnection

2. **Session Duration Limits (without compression):**
   - Gemini Live API: 15 minutes (audio-only) / 2 minutes (audio+video)
   - Vertex AI: 10 minutes (all sessions)

3. **No timeout/deadline parameters** in ADK RunConfig or run_live()

**Solution Implemented:** context_window_compression

Added to both RunConfig instances in server.py:790-838:
```python
context_window_compression=types.ContextWindowCompressionConfig(
    trigger_tokens=100000,
    sliding_window=types.SlidingWindow(target_tokens=80000),
)
```

**Benefits:**
- ‚úÖ **Unlimited session duration** (both Gemini and Vertex AI)
- ‚úÖ Works on current Gemini (AI Studio) environment
- ‚úÖ Official ADK best practice implementation
- ‚ö†Ô∏è Trade-off: Older conversation history summarized over time

**Reference:**
- https://google.github.io/adk-docs/streaming/dev-guide/part4/#streamingmode-bidi-or-sse
- "Session duration management and context window compression are Live API platform features"
- "With compression enabled, session duration becomes unlimited"

**Files Modified:**
- `server.py` - Added context_window_compression to RunConfig (lines 805-808, 834-837)

**Related Commit:**
- ffcb210 - Enable context_window_compression for unlimited BIDI sessions

**Testing Verification:**
- [x] Configuration applied to both native-audio and TEXT modalities
- [x] Server successfully starts with new configuration
- [x] Type checking and linting pass
- [ ] Manual testing: Long-duration streaming session (user verification pending)

**Impact:** Eliminates session duration limits, resolves "Deadline expired" errors for long sessions

---

### [P2-T2] WebSocket Bidirectional Communication - Client Events Implementation

**Status:** üü° In Progress (2025-12-13) - Phase 1-3 Complete ‚úÖ

**Investigation:** experiments/2025-12-13_bidirectional_protocol_investigation.md

**Summary:** Investigation revealed that protocol asymmetry is INTENTIONAL and correct:
- **Backend ‚Üí Frontend:** AI SDK v6 Data Stream Protocol (SSE format) - Required for useChat
- **Frontend ‚Üí Backend:** Simple JSON - No standard protocol exists, implementation freedom

However, user requirements justify implementing structured client events for:
1. ESC key interruption
2. CMD key audio control (BIDI)
3. Tool call approval dialogs

**Implementation Plan:** Unified Pattern (Constructor Options + Public Methods + Tool Callbacks)

---

#### Implementation Phases

**Phase 1: Foundation - Structured Event Protocol** ‚úÖ COMPLETE (1-2 hours)

**Goal:** Add type-safe event sending infrastructure

**Tasks:**
1. ‚úÖ Define `ClientToServerEvent` TypeScript types
2. ‚úÖ Add `sendEvent()` private method to WebSocketChatTransport
3. ‚úÖ Update existing message sending to use structured format
4. ‚úÖ Add public methods for user controls
5. ‚úÖ Update backend event handlers in server.py

**Files Modified:**
- `lib/websocket-chat-transport.ts` (+125 lines)
- `server.py` (~50 lines)

**Event Types to Define:**
```typescript
interface ClientEvent {
  type: string;
  version: "1.0";
  timestamp?: number;
}

interface MessageEvent extends ClientEvent {
  type: "message";
  data: { messages: UIMessage[] };
}

interface InterruptEvent extends ClientEvent {
  type: "interrupt";
  reason?: "user_abort" | "timeout" | "error";
}

interface AudioControlEvent extends ClientEvent {
  type: "audio_control";
  action: "start" | "stop";
}

interface ToolResultEvent extends ClientEvent {
  type: "tool_result";
  data: {
    toolCallId: string;
    result: any;
    status?: "approved" | "rejected";
  };
}

type ClientToServerEvent =
  | MessageEvent
  | InterruptEvent
  | AudioControlEvent
  | ToolResultEvent
  | PingEvent;
```

**Public Methods to Add:**
```typescript
public interrupt(reason?: string): void;
public startAudio(): void;
public stopAudio(): void;
public sendToolResult(toolCallId: string, result: any, status?: string): void;
```

**Acceptance Criteria:**
- [ ] All client events use structured format with `type` and `version`
- [ ] Public methods available for UI components
- [ ] Backward compatible with existing code
- [ ] Type-safe event sending

---

**Phase 2: Interruption Support** ‚úÖ COMPLETE (2-3 hours)

**Goal:** Enable ESC key to stop AI generation

**Tasks:**
1. ‚úÖ Export transport reference from buildUseChatOptions
2. ‚úÖ `interrupt()` public method already implemented in Phase 1
3. ‚úÖ Add ESC key handler in Chat component with useEffect
4. ‚úÖ Add visual interrupt indicator (red badge, 2s timeout)
5. ‚úÖ Backend: Handle `interrupt` event (close LiveRequestQueue) - Already implemented in Phase 1

**Files Modified:**
- `lib/build-use-chat-options.ts` - Export transport reference (~30 lines)
- `components/chat.tsx` - ESC key handler + interrupt indicator (~40 lines)

**Backend Handler:**
```python
elif event_type == "interrupt":
    logger.info("[BIDI] User interrupted (ESC)")
    live_request_queue.close()
    # ADK will handle graceful shutdown
```

**Frontend Handler:**
```typescript
useEffect(() => {
  const handleKeyDown = (e: KeyboardEvent) => {
    if (e.key === 'Escape') {
      transportRef.current?.interrupt('user_abort');
    }
  };
  window.addEventListener('keydown', handleKeyDown);
  return () => window.removeEventListener('keydown', handleKeyDown);
}, []);
```

**Acceptance Criteria:**
- [ ] ESC key stops current generation
- [ ] Backend receives and handles interrupt event
- [ ] UI shows feedback (toast/indicator)
- [ ] Works in both BIDI and non-BIDI modes

---

**Phase 3: Audio Control** ‚úÖ COMPLETE (4-6 hours + 2 hours AudioWorklet migration)

**Goal:** CMD key push-to-talk for BIDI audio input

**Tasks:**
1. ‚úÖ `startAudio()`, `stopAudio()`, `sendAudioChunk()` methods - Already implemented in Phase 1
2. ‚úÖ Integrated AudioWorklet for PCM recording (ADK official pattern)
3. ‚úÖ Added CMD key (Meta) handlers for push-to-talk
4. ‚úÖ Added visual recording indicator with pulse animation
5. ‚úÖ Backend: Forward PCM audio chunks to LiveRequestQueue

**AudioWorklet Implementation (Based on ADK Sample):**
- ‚úÖ Created `public/pcm-recorder-processor.js` - AudioWorklet processor
- ‚úÖ Created `lib/audio-recorder.ts` - AudioRecorder class
- ‚úÖ Replaced MediaRecorder with AudioWorklet in Chat component
- ‚úÖ 16kHz 16-bit PCM (matches ADK Live API requirements)
- ‚úÖ Echo cancellation, noise suppression, auto gain control

**Files Created:**
- `public/pcm-recorder-processor.js` - AudioWorklet processor (~50 lines)
- `lib/audio-recorder.ts` - AudioRecorder class (~200 lines)

**Files Modified:**
- `components/chat.tsx` - Audio recording + CMD key handlers (~120 lines)
- `server.py` - PCM audio_chunk handler (~30 lines)

**Audio Capture Flow:**
```typescript
// CMD down ‚Üí Start recording
transport.startAudio();
const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
const recorder = new MediaRecorder(stream);
recorder.ondataavailable = (e) => {
  transport.sendAudioChunk({
    content: arrayBufferToBase64(e.data),
    sampleRate: 24000,
    channels: 1,
    bitDepth: 16
  });
};

// CMD up ‚Üí Stop and auto-send
transport.stopAudio();
recorder.stop();
```

**Backend Handler:**
```python
elif event_type == "audio_chunk":
    chunk_data = base64.b64decode(event["data"]["chunk"])
    audio_blob = types.Blob(data=chunk_data, mime_type="audio/pcm")
    live_request_queue.send_realtime(blob=audio_blob)

elif event_type == "audio_control":
    action = event["action"]
    if action == "start":
        logger.info("[BIDI] Audio input started")
    elif action == "stop":
        logger.info("[BIDI] Audio input stopped (auto-send)")
```

**Open Questions:**
- [ ] Audio format requirements (PCM/WAV/sample rate)
- [ ] Browser MediaRecorder compatibility
- [ ] Fallback for unsupported browsers

**Acceptance Criteria:**
- [ ] CMD key starts/stops audio capture
- [ ] Audio chunks stream to backend in real-time
- [ ] Backend forwards to ADK LiveRequestQueue
- [ ] Works on Chrome/Safari/Firefox
- [ ] Visual feedback for recording state

---

**Phase 4: Tool Call Approval** (3-4 hours)

**Goal:** User approval dialogs for sensitive tool calls

**Tasks:**
1. Add `onToolCallRequest` callback (manual mode)
2. Update `handleToolCall()` to support both modes
3. Implement approval dialog UI
4. Add Geolocation API integration (example)

**Files to Modify:**
- `lib/websocket-chat-transport.ts` - Add approval mode
- `components/tool-approval-dialog.tsx` - New component
- `components/chat.tsx` - Integrate dialog

**Two Modes Support:**

**Mode 1: Auto-execute with approval (Simple)**
```typescript
toolCallCallback: async (toolCall) => {
  if (toolCall.toolName === 'change_bgm') {
    const approved = await showApprovalDialog({
      title: 'BGM Change',
      message: `Change to track ${toolCall.args.track}?`
    });
    return approved
      ? { success: true, track: toolCall.args.track }
      : { success: false, reason: 'User denied' };
  }
}
```

**Mode 2: Manual approval (Advanced)**
```typescript
onToolCallRequest: (toolCall) => {
  if (toolCall.toolName === 'get_location') {
    setToolApprovalDialog({
      toolCall,
      onApprove: async () => {
        const location = await getGeolocation();
        transport.sendToolResult(toolCall.toolCallId, location, "approved");
      },
      onDeny: () => {
        transport.sendToolResult(
          toolCall.toolCallId,
          { message: 'Êïô„Åà„Å™„ÅÑ„ÇàÔºÅ' },
          "rejected"
        );
      }
    });
  }
}
```

**Acceptance Criteria:**
- [ ] Both auto and manual approval modes work
- [ ] Approval dialog shows tool details
- [ ] Geolocation example works (approve/deny)
- [ ] Tool results sent back with status
- [ ] Backend handles rejected tools gracefully

---

#### Configuration Extension

**Updated WebSocketChatTransportConfig:**

```typescript
export interface WebSocketChatTransportConfig {
  url: string;
  timeout?: number;
  audioContext?: AudioContextValue;
  latencyCallback?: (latency: number) => void;

  // ===== Tool Call Handling =====
  toolCallCallback?: (toolCall: ToolCall) => Promise<any>;
  onToolCallRequest?: (toolCall: ToolCall) => void;

  // ===== Client Event Callbacks =====
  onInterruptRequest?: () => void;
  onAudioStart?: () => void;
  onAudioStop?: () => void;
  onAudioChunk?: (chunk: AudioChunk) => void;

  // ===== Connection Events =====
  onConnect?: () => void;
  onDisconnect?: () => void;
  onError?: (error: Error) => void;
}
```

---

#### Backend Event Handlers (server.py)

**Add to WebSocket message handler:**

```python
async def receive_from_client():
    while True:
        data = await websocket.receive_text()
        event = json.loads(data)
        event_type = event.get("type")

        if event_type == "message":
            # Existing message handling
            messages = event["data"]["messages"]
            # ...

        elif event_type == "interrupt":
            # NEW: Handle interruption
            logger.info("[BIDI] User interrupted")
            live_request_queue.close()

        elif event_type == "audio_control":
            # NEW: Handle audio start/stop
            action = event["action"]
            logger.info(f"[BIDI] Audio {action}")

        elif event_type == "audio_chunk":
            # NEW: Handle audio streaming
            chunk_data = base64.b64decode(event["data"]["chunk"])
            audio_blob = types.Blob(data=chunk_data, mime_type="audio/pcm")
            live_request_queue.send_realtime(blob=audio_blob)

        elif event_type == "tool_result":
            # NEW: Handle tool results with approval status
            tool_call_id = event["data"]["toolCallId"]
            result = event["data"]["result"]
            status = event["data"].get("status", "approved")
            logger.info(f"[Tool] User {status} tool {tool_call_id}")
            # TODO: Investigate ADK tool result handling

        elif event_type == "ping":
            # Existing ping/pong
            await websocket.send_text(json.dumps({
                "type": "pong",
                "timestamp": event["timestamp"]
            }))
```

---

#### Testing Strategy

**Unit Tests:**
- [ ] Event type definitions and serialization
- [ ] Public method error handling
- [ ] Callback invocation

**Integration Tests:**
- [ ] Full round-trip for each event type
- [ ] Backend event handling
- [ ] Error scenarios

**E2E Tests:**
- [ ] ESC key interruption flow
- [ ] CMD key audio capture flow
- [ ] Tool approval flow (approve/deny)

---

#### Implementation Timeline

**Total Estimated Effort:** 10-14 hours

- Phase 1: 1-2 hours ‚úì Foundation (immediate value)
- Phase 2: 2-3 hours ‚úì High value, low risk
- Phase 3: 4-6 hours ‚ö†Ô∏è Medium risk (browser APIs)
- Phase 4: 3-4 hours ‚ö†Ô∏è Medium complexity

**Recommended Start:** Phase 1 + Phase 2 (interruption) for quick wins

---

#### Success Metrics

- ‚úÖ ESC key reliably stops generation
- ‚úÖ CMD key audio input works in BIDI mode
- ‚úÖ Tool approval dialogs functional
- ‚úÖ All events use structured format
- ‚úÖ Backward compatible with existing code
- ‚úÖ Type-safe throughout

---

**Reference:**
- Investigation: experiments/2025-12-13_bidirectional_protocol_investigation.md
- Community pattern: https://github.com/vercel/ai/discussions/5607
- AI SDK v6 Transport docs: https://ai-sdk.dev/docs/ai-sdk-ui/transport

---

### [P2-T3] Immediate Error Detection (errorCode/errorMessage)

**Issue:** Error detection happens too late in stream processing

**Current Behavior:**
- `errorCode` and `errorMessage` only checked in `finalize()` method
- Called at **end of stream** or when exception occurs
- Mid-stream errors (quota exceeded, model errors) may not be detected immediately

**Problem:**
```python
# stream_protocol.py - Current flow
async def stream_adk_to_ai_sdk():
    try:
        async for event in event_stream:
            async for sse_event in converter.convert_event(event):  # ‚Üê errorCode NOT checked here
                yield sse_event
    finally:
        async for final_event in converter.finalize(error=error):  # ‚Üê errorCode checked here (too late)
            yield final_event
```

**Required Fix:**

Add error detection at the **start** of `convert_event()`:

```python
# stream_protocol.py:~180 (in convert_event method)
async def convert_event(self, event: Event) -> AsyncGenerator[str, None]:
    # Check for errors FIRST (before any other processing)
    if hasattr(event, "error_code") and event.error_code:
        error_message = getattr(event, "error_message", "Unknown error")
        logger.error(f"[ERROR] ADK error detected: {event.error_code} - {error_message}")

        # Send error event immediately
        yield self._format_sse_event({
            "type": "error",
            "error": {
                "code": event.error_code,
                "message": error_message
            }
        })
        return  # Stop processing this event

    # ... rest of convert_event processing
```

**Testing Requirements:**
- [ ] Test with simulated ADK error event (error_code set)
- [ ] Verify error event sent immediately (not delayed until finalize)
- [ ] Verify subsequent events after error are handled correctly
- [ ] Add unit test: `test_convert_event_with_error_code()`

**Priority:** üî¥ HIGH - Critical for proper error handling

**Impact:** Users will see errors immediately instead of waiting for stream to finish

**Reference:**
- experiments/2025-12-12_adk_field_mapping_completeness.md - Section "Immediate Actions (Priority 1)"
- Coverage analysis: errorCode/errorMessage marked as "‚ö†Ô∏è Partial" (only in finalize)

---

### [P2-T4] Field Coverage Testing (Automated CI Checks)

**Issue:** New ADK fields may be added without detection (example: `output_transcription` was discovered by accident)

**Goal:** Automatically detect when ADK SDK adds new fields that we're not handling

**Implementation:** Create `tests/unit/test_field_coverage.py`

**Test Strategy:**

```python
"""
Test to ensure all ADK Event/Part fields are accounted for.
This test should FAIL when ADK SDK adds new fields.
"""
import inspect
from google.adk.events import Event
from google.genai import types

# Known fields we handle (update when implementing new fields)
IMPLEMENTED_EVENT_FIELDS = {
    "content", "turnComplete", "usageMetadata", "finishReason",
    "outputTranscription",
}

# Fields we know about but haven't implemented yet (with justification)
DOCUMENTED_EVENT_FIELDS = {
    "errorCode": "TODO: Add immediate detection in convert_event()",
    "errorMessage": "TODO: Add immediate detection in convert_event()",
    "inputTranscription": "Low priority: Client already has user input",
    "groundingMetadata": "Awaiting UI design decision",
    "citationMetadata": "Awaiting UI design decision",
    "interrupted": "Medium priority: BIDI UX feature",
    # ... complete list with justifications
}

# Internal/metadata fields (low priority, document why skipped)
METADATA_FIELDS = {
    "author", "id", "timestamp", "invocationId", "branch",
    "actions", "longRunningToolIds", "partial", "modelVersion",
    "avgLogprobs", "logprobsResult", "cacheMetadata",
    "liveSessionResumptionUpdate", "customMetadata"
}

def test_event_field_coverage():
    """Verify all Event fields are either implemented or documented as TODO."""
    event_sig = inspect.signature(Event)
    all_fields = set(event_sig.parameters.keys())

    known_fields = IMPLEMENTED_EVENT_FIELDS | set(DOCUMENTED_EVENT_FIELDS.keys()) | METADATA_FIELDS
    unknown_fields = all_fields - known_fields

    assert not unknown_fields, (
        f"üö® New ADK Event fields detected: {unknown_fields}\n"
        f"Action required:\n"
        f"1. Review new fields in ADK SDK documentation\n"
        f"2. Decide: Implement now, document as TODO, or mark as metadata\n"
        f"3. Update IMPLEMENTED_EVENT_FIELDS, DOCUMENTED_EVENT_FIELDS, or METADATA_FIELDS\n"
        f"4. Update experiments/2025-12-12_adk_field_mapping_completeness.md\n"
    )

def test_part_field_coverage():
    """Verify all Part fields are accounted for."""
    # Similar pattern for Part fields
    part_sig = inspect.signature(types.Part)
    all_fields = set(part_sig.parameters.keys())

    # ... similar logic for Part fields
```

**CI Integration:**

Add to `.github/workflows/test.yaml` (or equivalent):
```yaml
- name: Check ADK field coverage
  run: |
    uv run pytest tests/unit/test_field_coverage.py -v
```

**Maintenance Workflow:**

When test fails (ADK SDK updated):
1. ‚úÖ CI fails with "New ADK fields detected" message
2. üîç Review new fields in ADK documentation
3. üìä Update coverage analysis in experiments/2025-12-12_adk_field_mapping_completeness.md
4. üí¨ Discuss AI SDK v6 mapping strategy
5. ‚úÖ Implement high-priority fields OR
6. üìù Document as TODO with justification
7. ‚úÖ Update test field lists
8. ‚úÖ CI passes again

**Testing Requirements:**
- [ ] Test passes with current ADK SDK version
- [ ] Test fails when unknown field added (simulate by removing field from known list)
- [ ] Error message is clear and actionable
- [ ] Similar test for Part fields
- [ ] CI integration working

**Priority:** üî¥ HIGH - Prevents missing fields in future updates

**Impact:** Proactive detection of new ADK features instead of accidental discovery

**Reference:**
- experiments/2025-12-12_adk_field_mapping_completeness.md - Section "Automated Completeness Checking"
- scripts/check-coverage.py - Current coverage detection (34.2%)

---

### [P2-T5] Tool Error Handling - ‚úÖ Complete

**Status:** ‚úÖ COMPLETE (2025-12-12)

**Implementation Summary:**

Backend implementation was already complete (stream_protocol.py:504-512):
- `tool-output-error` event generation with correct `errorText` field
- Error detection logic for tool execution failures
- Proper logging of tool errors

Frontend issue discovered and fixed:
- **Problem:** `components/tool-invocation.tsx` used incorrect field name `error` instead of `errorText`
- **Root Cause:** Component used `any` type, bypassing TypeScript type checking
- **Fix:** Changed field reference to `errorText` (commit a7d3bcf)
- **Documentation:** Added comprehensive JSDoc describing AI SDK v6 expected structure

**Comprehensive Audit Results:**

All components audited for AI SDK v6 field compliance:
- ‚úÖ `components/message.tsx` - All fields correct (`input`, `output`, `toolCallId`, `state`)
- ‚úÖ `components/image-upload.tsx` - No AI SDK field issues
- ‚úÖ `components/image-display.tsx` - No AI SDK field issues
- ‚úÖ `components/audio-player.tsx` - No AI SDK field issues
- ‚úÖ `components/chat.tsx` - No AI SDK field issues
- ‚úÖ `lib/websocket-chat-transport.ts` - All fields correct (`toolCallId`, `errorText`)
- ‚úÖ `lib/websocket-chat-transport.test.ts` - Tests use correct `errorText`
- ‚úÖ `app/api/chat/route.ts` - No AI SDK field issues

**Files Modified:**
- `components/tool-invocation.tsx` - Fixed field name and added documentation

**Related Commits:**
- a7d3bcf - Update ToolInvocationComponent to use errorText for output-error state

**Testing Verification:**
- [x] Backend generates `tool-output-error` with `errorText` field
- [x] Frontend component displays error correctly
- [x] All components use correct AI SDK v6 field names
- [x] Build passes with no TypeScript errors

**Impact:** Tool errors are now properly displayed to users with correct field naming across entire codebase

---

### [P2-T6] Unify Image Events to `file` Type

**Status:** ‚úÖ Complete (Implemented 2025-12-12)

**Implementation:**

**1. Backend (stream_protocol.py:608-620):**
```python
# Process image data
if mime_type.startswith("image/"):
    # Convert bytes to base64 string
    base64_content = base64.b64encode(inline_data.data).decode("utf-8")

    # Use AI SDK v6 standard 'file' event with data URL
    # This matches the input format (symmetric input/output)
    event = self._format_sse_event({
        "type": "file",
        "url": f"data:{mime_type};base64,{base64_content}",
        "mediaType": mime_type,
    })
    logger.debug(f"[IMAGE OUTPUT] mime_type={mime_type}, size={len(inline_data.data)} bytes")
    return [event]
```

**2. Frontend (message.tsx:221-235):**
```typescript
// File content - Image (AI SDK v6 file part)
if (part.type === "file" && part.mediaType?.startsWith("image/")) {
  return (
    <img
      key={index}
      src={part.url}
      alt={part.filename || "Image"}
      style={{ maxWidth: "100%", borderRadius: "8px", margin: "0.5rem 0" }}
    />
  );
}
```

**3. Coverage Verification:**
- ‚úÖ Backend generates `file` events (confirmed by `just check-coverage-verbose`)
- ‚úÖ Frontend handles `file` events (confirmed by coverage report)

**Results:**
- ‚úÖ Symmetric input/output format achieved
- ‚úÖ AI SDK v6 standard compliance
- ‚úÖ Backward compatibility maintained (legacy `data-image` handling still present)

**Commit:** 05161a7 (Dec 12, 2025)

---

### [P2-T7] Audio Completion Signaling - Frontend Integration

**Status:** ‚úÖ Complete (Implemented 2025-12-12)

**Implementation:**

**1. Backend (stream_protocol.py:684-701):**
```python
# finalize() method sends finish event with messageMetadata.audio
async def finalize(self, usage_metadata, error, finish_reason, grounding_metadata):
    # ... existing code ...

    # Audio completion metadata
    if self.pcm_chunk_count > 0:
        audio_duration = self.pcm_total_bytes / (self.pcm_sample_rate * 2)
        message_metadata["audio"] = {
            "chunks": self.pcm_chunk_count,
            "bytes": self.pcm_total_bytes,
            "sampleRate": self.pcm_sample_rate,
            "duration": audio_duration,
        }
```

**2. AudioContext (lib/audio-context.tsx:41-50, 345-348, 404-405):**
```typescript
interface AudioContextValue {
  voiceChannel: {
    isPlaying: boolean;
    chunkCount: number;
    sendChunk: (chunk: PCMChunk) => void;
    reset: () => void;
    onComplete: (metadata: AudioMetadata) => void;  // ‚úÖ Implemented
    lastCompletion: AudioMetadata | null;           // ‚úÖ Implemented
  };
}

const handleAudioComplete = (metadata: AudioMetadata) => {
  setLastCompletion(metadata);
  setIsPlaying(false);
};
```

**3. WebSocket Transport (lib/websocket-chat-transport.ts:548-550):**
```typescript
if (chunk.type === "finish" && chunk.messageMetadata?.audio) {
  // Log completion statistics
  console.log("[Audio Stream] Audio streaming completed");

  // Notify AudioContext of audio completion
  if (this.config.audioContext?.voiceChannel?.onComplete) {
    this.config.audioContext.voiceChannel.onComplete(metadata.audio);
  }
}
```

**4. UI Display (components/chat.tsx:140-167):**
```typescript
{/* Audio Completion Indicator (BIDI mode only) */}
{mode === "adk-bidi" && audioContext.voiceChannel.lastCompletion && (
  <div style={{ position: "fixed", bottom: "1rem", right: "1rem" }}>
    <span>‚úì</span>
    <span>
      Audio: {audioContext.voiceChannel.lastCompletion.duration.toFixed(2)}s
      ({audioContext.voiceChannel.lastCompletion.chunks} chunks)
    </span>
  </div>
)}
```

**Results:**
- ‚úÖ Backend sends audio completion metadata in finish event
- ‚úÖ AudioContext receives and stores completion data
- ‚úÖ UI displays completion indicator with duration and chunk count
- ‚úÖ Works correctly with multiple audio responses in same session

**Commit:** 005bd81 (Dec 12, 2025)

**Related:** Experiment 2025-12-12_audio_stream_completion_notification.md

---

### [P2-T8] message-metadata Event Implementation - ‚úÖ Complete

**Status:** ‚úÖ COMPLETE (2025-12-13)

**Original Issue:** ADK metadata fields (grounding, citations, cache, etc.) were not forwarded to frontend

**Implementation Summary:**

**Backend (Already Complete):**
- stream_protocol.py:628-775 - `finalize()` method accepts all metadata parameters
- stream_protocol.py:782-860 - `stream_adk_to_ai_sdk()` collects metadata from events
- All metadata included in `finish` event's `messageMetadata` field

**Frontend (Implemented):**
- components/message.tsx:417-548 - Added UI display for all metadata fields
- Follows existing usage metadata display pattern
- Consistent styling and conditional rendering

**Implemented Metadata Fields:**

1. **Grounding Sources (RAG, Web Search)** - lines 417-457
   ```tsx
   üîç Sources (N):
     - Title/URI as clickable links
     - Opens in new tab
   ```

2. **Citations** - lines 459-504
   ```tsx
   üìù Citations (N):
     - [startIndex-endIndex] URI
     - Optional license information
   ```

3. **Cache Metadata** - lines 506-529
   ```tsx
   üíæ Cache: N hits / N misses
     - Green for hits, Red for misses
   ```

4. **Model Version** - lines 531-548
   ```tsx
   ü§ñ Model: model-name
   ```

5. **Usage Metadata** - lines 387-415 (Already existed)
   ```tsx
   üìä Tokens: N in + N out = N total
   ```

**Files Modified:**
- `components/message.tsx` - Added metadata display UI

**Related Commit:**
- 0916c58 - Add UI display for all messageMetadata fields

**Testing Verification:**
- [x] Backend collects all metadata from ADK events
- [x] Metadata forwarded in finish event's messageMetadata
- [x] Frontend console logs metadata (websocket-chat-transport.ts:531-583)
- [x] UI displays all metadata fields when available
- [x] Build succeeds with no TypeScript errors
- [ ] Manual testing: Verify with actual RAG/grounding/citation data (requires ADK features)

**Impact:** Users can now see grounding sources, citations, cache statistics, and model version in the chat UI

**Note:** Some ADK metadata fields not yet implemented:
- `customMetadata` - User-defined metadata (ADK feature not yet used)
- `inputTranscription` - User audio transcription (separate feature)
- `logprobsResult`, `avgLogprobs` - Token probabilities (not in current ADK events)

These can be added when ADK starts providing them.

---

**Original Implementation Strategy (for reference):**

**Phase 1: Add metadata to `finish` event (Priority 1)**

```python
# stream_protocol.py:507-560 (extend finalize method)
async def finalize(
    self,
    usage_metadata: Any | None = None,
    error: Exception | None = None,
    finish_reason: Any | None = None,
    grounding_metadata: Any | None = None,  # NEW
    citation_metadata: Any | None = None,   # NEW
    cache_metadata: Any | None = None,      # NEW
    model_version: str | None = None,       # NEW
) -> AsyncGenerator[str, None]:
    # ... existing code ...

    # Build messageMetadata
    metadata: dict[str, Any] = {}

    # Add usage
    if usage_metadata:
        metadata["usage"] = { ... }

    # Add audio stats
    if self.pcm_chunk_count > 0:
        metadata["audio"] = { ... }

    # Add grounding sources (NEW)
    if grounding_metadata:
        sources = []
        for chunk in grounding_metadata.grounding_chunks or []:
            if hasattr(chunk, 'web'):
                sources.append({
                    "type": "web",
                    "uri": chunk.web.uri,
                    "title": chunk.web.title,
                })
        if sources:
            metadata["grounding"] = {"sources": sources}

    # Add citations (NEW)
    if citation_metadata:
        citations = []
        for source in citation_metadata.citation_sources or []:
            citations.append({
                "startIndex": source.start_index,
                "endIndex": source.end_index,
                "uri": source.uri,
                "license": source.license,
            })
        if citations:
            metadata["citations"] = citations

    # Add cache metadata (NEW)
    if cache_metadata:
        metadata["cache"] = {
            "hits": getattr(cache_metadata, "cache_hits", 0),
            "misses": getattr(cache_metadata, "cache_misses", 0),
        }

    # Add model version (NEW)
    if model_version:
        metadata["modelVersion"] = model_version

    if metadata:
        finish_event["messageMetadata"] = metadata

    yield self._format_sse_event(finish_event)
```

**Phase 2: Collect metadata from events (Priority 1)**

```python
# stream_protocol.py:564-616 (update stream_adk_to_ai_sdk)
async def stream_adk_to_ai_sdk(event_stream):
    # ... existing code ...

    # Track metadata from events
    grounding_metadata = None
    citation_metadata = None
    cache_metadata = None
    model_version = None

    try:
        async for event in event_stream:
            # Collect metadata fields
            if hasattr(event, "grounding_metadata") and event.grounding_metadata:
                grounding_metadata = event.grounding_metadata
            if hasattr(event, "citation_metadata") and event.citation_metadata:
                citation_metadata = event.citation_metadata
            if hasattr(event, "cache_metadata") and event.cache_metadata:
                cache_metadata = event.cache_metadata
            if hasattr(event, "model_version") and event.model_version:
                model_version = event.model_version

            # ... existing conversion logic ...
    finally:
        # Send finalize with all collected metadata
        async for final_event in converter.finalize(
            usage_metadata=usage_metadata,
            error=error,
            finish_reason=finish_reason,
            grounding_metadata=grounding_metadata,
            citation_metadata=citation_metadata,
            cache_metadata=cache_metadata,
            model_version=model_version,
        ):
            yield final_event
```

**Phase 3: Standalone message-metadata events (Priority 2)**

For streaming updates during generation (optional):

```python
# stream_protocol.py:convert_event (add metadata event generation)
async def convert_event(self, event: Event) -> AsyncGenerator[str, None]:
    # ... existing error check ...

    # Send grounding metadata as standalone event (if mid-stream)
    if hasattr(event, "grounding_metadata") and event.grounding_metadata:
        # Only send as standalone if not in finalize (to avoid duplication)
        if not hasattr(event, "turn_complete") or not event.turn_complete:
            sources = [...]  # Extract sources
            if sources:
                yield self._format_sse_event({
                    "type": "message-metadata",
                    "messageMetadata": {
                        "grounding": {"sources": sources}
                    }
                })
```

**Testing Requirements:**
- [ ] Test with ADK response containing groundingMetadata
- [ ] Test with ADK response containing citationMetadata
- [ ] Test with ADK response containing cacheMetadata
- [ ] Verify `finish` event includes messageMetadata
- [ ] Verify frontend can access metadata
- [ ] Add unit tests for metadata extraction

**Priority:** üü° MEDIUM - Enhances transparency, not critical for basic functionality

**Impact:** Frontend can display grounding sources, citations, cache stats

**Reference:**
- experiments/2025-12-12_adk_field_mapping_completeness.md - Section "High-Value Fields"
- AI SDK v6 UIMessageChunk type (message-metadata event)

---

## Phase 3: Êñ∞Ê©üËÉΩÊ§úË®é

### [P3-T1] Live API Transcriptions - ‚úÖ Complete

**Status:** ‚úÖ COMPLETE (2025-12-13)

**Original Goal:** Support Live API transcription features for audio input/output

**Implementation Summary:**

Live API Transcriptions provide text transcription for audio in BIDI mode:
- **Input Transcription**: User speaks ‚Üí ADK recognizes speech ‚Üí text displayed
- **Output Transcription**: AI speaks ‚Üí Native-audio model transcribes ‚Üí text displayed

**Backend (stream_protocol.py):**

1. **Input Transcription Handler** - Lines 308-349
   ```python
   # User audio input ‚Üí text (ADK BIDI mode)
   if hasattr(event, "input_transcription") and event.input_transcription:
       # Convert to AI SDK v6 text-start/text-delta/text-end events
   ```
   - Receives `input_transcription` events from ADK Live API
   - Converts to AI SDK v6 text events (text-start ‚Üí text-delta ‚Üí text-end)
   - Tracks state with `_input_text_block_started` flag

2. **Output Transcription Handler** - Lines 351-389
   ```python
   # AI audio output ‚Üí text (native-audio models)
   if hasattr(event, "output_transcription") and event.output_transcription:
       # Convert to AI SDK v6 text-start/text-delta/text-end events
   ```
   - Receives `output_transcription` events from native-audio models
   - Same AI SDK v6 text event conversion
   - Tracks state with `_output_text_block_started` flag

**Server Configuration (server.py):**
- Lines 800-801: Native-audio config with AudioTranscriptionConfig
  ```python
  input_audio_transcription=types.AudioTranscriptionConfig(),
  output_audio_transcription=types.AudioTranscriptionConfig(),
  ```
- Lines 829-830: TEXT modality config (transcription disabled)
  ```python
  input_audio_transcription=None,
  output_audio_transcription=None,
  ```

**Frontend (lib/websocket-chat-transport.ts):**
- Lines 337-348: Standard event forwarding
  - Transcription text events forwarded to AI SDK useChat hook
  - No special handling needed (standard text-delta events)

**UI Display (components/message.tsx):**
- Lines 166-182: Text content rendering
  - Transcription text displayed in regular message text blocks
  - No distinction between typed text and transcribed audio text
  - Same styling and layout as normal text responses

**Data Flow:**

Input Transcription:
```
User speaks ‚Üí ADK Live API ‚Üí input_transcription event
‚Üí Backend: text-start/text-delta/text-end
‚Üí WebSocket ‚Üí Frontend: text events
‚Üí UI: Display as message text
```

Output Transcription:
```
AI audio response ‚Üí Native-audio model ‚Üí output_transcription event
‚Üí Backend: text-start/text-delta/text-end
‚Üí WebSocket ‚Üí Frontend: text events
‚Üí UI: Display as message text
```

**Documentation Added:**
- stream_protocol.py: Comprehensive comment blocks explaining both transcription handlers
- websocket-chat-transport.ts: Comments clarifying transcription event forwarding
- message.tsx: Comments linking text display to transcription features

**Files Modified:**
- `stream_protocol.py` - Added documentation comments (lines 308-363)
- `lib/websocket-chat-transport.ts` - Added documentation comments (lines 339-342)
- `components/message.tsx` - Added documentation comments (lines 166-169)

**Related Commits:**
- 92bb8e5 - Add clarifying comments to transcription implementation

**Testing Verification:**
- [x] Input transcription handler implemented and documented
- [x] Output transcription handler implemented and documented
- [x] Server configuration correct for both modalities
- [x] Frontend forwarding works (standard text events)
- [x] UI displays transcription text correctly
- [ ] Manual testing: Verify with actual ADK Live API audio input (user verification pending)
- [ ] Manual testing: Verify with native-audio model output (user verification pending)

**Impact:** Users can see text transcriptions of both their spoken input and AI's audio responses in real-time

**Note:** This feature is only active when:
- Using BIDI mode (WebSocket connection)
- AudioTranscriptionConfig is enabled in server configuration
- For output transcription: Using native-audio models (AUDIO modality)

---

### [P3-T2] Grounding & Citation Metadata - ‚úÖ Complete

**Status:** ‚úÖ COMPLETE (2025-12-13) - Implemented as part of [P2-T8]

**Original Goal:** Display grounding sources and citations in the UI

**Implementation:**
This task was completed as part of P2-T8 message-metadata Event Implementation.

**Backend (stream_protocol.py):**
- Lines 714-732: Grounding metadata collection and formatting
  - Extracts web search results, RAG sources
  - Formats as `{"type": "web", "uri": "...", "title": "..."}`
- Lines 735-752: Citation metadata collection and formatting
  - Extracts citation sources with index ranges
  - Formats as `{"startIndex": N, "endIndex": N, "uri": "...", "license": "..."}`

**Frontend (components/message.tsx):**
- Lines 417-457: üîç Grounding Sources display
  - Shows source count
  - Clickable links with titles
  - Opens in new tab
- Lines 459-504: üìù Citations display
  - Shows citation count
  - [startIndex-endIndex] URI format
  - Optional license information display

**User Experience:**
```
üîç Sources (3):
  - Search result title
  - Another source
  - RAG document

üìù Citations (2):
  - [0-150] https://example.com/source1
  - [200-350] https://example.com/source2 (CC BY 4.0)
```

**Related:**
- [P2-T8] message-metadata Event Implementation - Parent task
- Commits: 0916c58, add5f47

**Impact:** Users can now see and verify the sources and citations used by the AI model

---
